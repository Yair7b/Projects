{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BigData Project.ipynb","provenance":[],"collapsed_sections":["HblxI4LZknTB","PA3EOAkD5Y3c","bE4Wah3OkpOJ","zm3wFdTZNuUC","BUJnZARRoOLJ"],"authorship_tag":"ABX9TyOwS+SXFOx213AYbf/Tw+fj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Big Data Project**"],"metadata":{"id":"hEB0ZQhekYDF"}},{"cell_type":"markdown","source":["**Yair Barel  •  Shlomi Kiko  •  Ori Valdman**"],"metadata":{"id":"yn-V0_2Skh_c"}},{"cell_type":"markdown","source":["***\n","***\n","***"],"metadata":{"id":"DBwCPmFdklOp"}},{"cell_type":"markdown","source":["### **Install packages and import libraries**"],"metadata":{"id":"HblxI4LZknTB"}},{"cell_type":"code","source":["!apt update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n","!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n","!pip install -q findspark\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n","\n","import findspark\n","findspark.init(\"spark-3.1.2-bin-hadoop3.2\")\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","from pyspark.sql import Row\n","from pyspark.sql import functions\n","from pyspark.sql.functions import *\n","import pyspark"],"metadata":{"id":"fDFy8r5Akb2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"3EPqqNfGUkJg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Rating Table**"],"metadata":{"id":"PA3EOAkD5Y3c"}},{"cell_type":"code","source":["# import csv\n","df_rating = spark.read.option(\"delimiter\", \";\").option(\"quote\",\"\").csv('/content/drive/MyDrive/BDA/BigData/BX-Book-Ratings.csv',inferSchema = True, header = True)\n","\n","# clean and rename the columns titles\n","df_rating=df_rating.withColumnRenamed('\"User-ID','user_id')\n","df_rating=df_rating.withColumnRenamed('\"\"ISBN\"\"','isbn')\n","df_rating=df_rating.withColumnRenamed('\"\"Book-Rating\"\"\",,','rating')\n","\n","# clean the rows\n","df_rating = df_rating.withColumn('user_id', regexp_replace('user_id', '[^A-Z0-9_]', ''))\n","df_rating = df_rating.withColumn('isbn', regexp_replace('isbn', '[^A-Z0-9_]', ''))\n","df_rating = df_rating.withColumn('rating', regexp_replace('rating', '[^A-Z0-9_]', ''))\n","\n","# replace empty values with null\n","df_rating = df_rating.withColumn('isbn', when(col('isbn') == '', None).otherwise(col('isbn')))\n","\n","# drop all null from the table \n","df_rating = df_rating.na.drop()\n","\n","# drop rows with defective ISBN values\n","df_rating = df_rating.filter('(isbn rlike \"^[0-9]+$\") or (isbn rlike \"^[0-9]+X$\")')\n","df_rating = df_rating.filter(length(col(\"isbn\")) == 10)\n","\n","# trim trail and leading blank spaces\n","df_rating = df_rating.withColumn('user_id', trim(df_rating.user_id))\n","df_rating = df_rating.withColumn('isbn', trim(df_rating.isbn))\n","df_rating = df_rating.withColumn('rating', trim(df_rating.rating))\n","\n","# convert data type to INT\n","df_rating = df_rating.withColumn(\"user_id\", df_rating.user_id.cast('int'))\n","df_rating = df_rating.withColumn(\"rating\", df_rating.rating.cast('int'))\n","\n","# remove duplicate rows (found 4 duplicates)\n","df_rating = df_rating.distinct()"],"metadata":{"id":"txoM6hYZ5ZIN","executionInfo":{"status":"ok","timestamp":1649798247238,"user_tz":-180,"elapsed":2070,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["df_rating.show(10,truncate = False)\n","df_rating.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-llXW3EGdNY","executionInfo":{"status":"ok","timestamp":1649798279537,"user_tz":-180,"elapsed":29874,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}},"outputId":"1a186f16-5cd6-49c9-dc8a-21ac0d408bbc"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+----------+------+\n","|user_id|isbn      |rating|\n","+-------+----------+------+\n","|276875 |8806155873|0     |\n","|277042 |0446611212|8     |\n","|277187 |038528859X|0     |\n","|277195 |006251170X|0     |\n","|277427 |042514755X|0     |\n","|277427 |0915943263|0     |\n","|277427 |0930014464|0     |\n","|277478 |034066049X|0     |\n","|277478 |0440234743|0     |\n","|277478 |0860685179|0     |\n","+-------+----------+------+\n","only showing top 10 rows\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["1038353"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","source":["# export df_rating into csv\n","df_rating.repartition(1).write.csv(\"/content/drive/MyDrive/CSV/df_rating.csv\", header = True)"],"metadata":{"id":"QQtoBp3qGiZn","executionInfo":{"status":"ok","timestamp":1649798300523,"user_tz":-180,"elapsed":15881,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":["### **Books Table**"],"metadata":{"id":"bE4Wah3OkpOJ"}},{"cell_type":"code","source":["# import csv\n","df_book = spark.read.option(\"delimiter\", \";\").csv('/content/drive/MyDrive/BDA/BigData/BX-Books.csv',inferSchema = True, header = True)\n","\n","# drop irrelevant columns\n","df_book = df_book.drop(\"Image-URL-S\",\"Image-URL-M\",\"Image-URL-L\")\n","\n","# change the columns name\n","df_book = df_book.withColumnRenamed(\"ISBN\",\"isbn\") \\\n","      .withColumnRenamed(\"Book-Title\",\"book_title\") \\\n","      .withColumnRenamed(\"Book-Author\",\"book_author\") \\\n","      .withColumnRenamed(\"Year-Of-Publication\",\"publication_year\") \\\n","      .withColumnRenamed(\"Publisher\",\"publisher\")\n","\n","# drop rows with defective ISBN values\n","df_book = df_book.filter('(isbn rlike \"^[0-9]+$\") or (isbn rlike \"^[0-9]+X$\")')\n","df_book = df_book.filter(length(col(\"isbn\")) == 10)\n","\n","# publication_year - replace defective values into null\n","df_book = df_book.withColumn('publication_year', when(col('publication_year') == '', None).otherwise(col('publication_year')))\n","df_book = df_book.withColumn('publication_year', when(length('publication_year') != 4, None).otherwise(col('publication_year')))\n","df_book = df_book.withColumn('publication_year', when(col('publication_year') > 2022, None).otherwise(col('publication_year')))\n","\n","# publication_year - filter numbers only and null\n","df_book = df_book.na.fill({\"publication_year\": -1})\n","\n","# drop rows that have both NULL in book_author and numbers in book_title\n","df_book = df_book.filter('(book_title not rlike \"^[0-9]+$\") and (book_author is not null)')\n","\n","# filter out special character from columns\n","df_book = df_book.withColumn('book_title', regexp_replace('book_title', '[$&+,:;=?@#|<>^*()%!�\"]', ''))\n","df_book = df_book.withColumn('book_author', regexp_replace('book_author', '[$&+,:;=?@#|<>^*()%!�\"]', ''))\n","df_book = df_book.withColumn('publisher', regexp_replace('publisher', '[$&+,:;=?@#|<>^*()%!�\"]', ''))\n","\n","# trim trail and leading blank spaces\n","df_book = df_book.withColumn('isbn', trim(df_book.isbn))\n","df_book = df_book.withColumn('book_title', trim(df_book.book_title))\n","df_book = df_book.withColumn('book_author', trim(df_book.book_author))\n","df_book = df_book.withColumn('publication_year', trim(df_book.publication_year))\n","df_book = df_book.withColumn('publisher', trim(df_book.publisher))\n","\n","# convert data type to INT\n","df_book = df_book.withColumn(\"publication_year\", df_book.publication_year.cast('int'))\n","\n","# lowercase all values\n","df_book.createOrReplaceTempView('df_book')\n","df_book = spark.table('df_book').withColumn('book_title', lower(col('book_title')))\n","\n","df_book.createOrReplaceTempView('df_book')\n","df_book = spark.table('df_book').withColumn('book_author', lower(col('book_author')))\n","\n","df_book.createOrReplaceTempView('df_book')\n","df_book = spark.table('df_book').withColumn('publisher', lower(col('publisher')))\n","\n","# convert null-like values into NULL\n","Dict = {'n/a':None,'NULL':None,'':None,'None':None,' ':None,'null':None,'nan':None,'none':None}\n","df_book = df_book.replace(Dict,subset=['book_title','book_author','publisher'])\n","\n","# convert NULL to unknown\n","df_book = df_book.na.fill({\"book_title\": \"unknown\", \"book_author\": \"unknown\", \"publisher\": \"unknown\"})"],"metadata":{"id":"lJo_M7EP0UM2","executionInfo":{"status":"ok","timestamp":1649798317341,"user_tz":-180,"elapsed":2275,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["df_book.show(10,truncate = False)\n","df_book.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8EBxw4OOhR6","executionInfo":{"status":"ok","timestamp":1649798321944,"user_tz":-180,"elapsed":1677,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}},"outputId":"5e0410dd-082a-4807-b725-d557076f3f8d"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+-------------------------------------------------------------------------------------------------+--------------------+----------------+------------------------+\n","|isbn      |book_title                                                                                       |book_author         |publication_year|publisher               |\n","+----------+-------------------------------------------------------------------------------------------------+--------------------+----------------+------------------------+\n","|0195153448|classical mythology                                                                              |mark p. o. morford  |2002            |oxford university press |\n","|0002005018|clara callan                                                                                     |richard bruce wright|2001            |harperflamingo canada   |\n","|0060973129|decision in normandy                                                                             |carlo d'este        |1991            |harperperennial         |\n","|0374157065|flu the story of the great influenza pandemic of 1918 and the search for the virus that caused it|gina bari kolata    |1999            |farrar straus giroux    |\n","|0393045218|the mummies of urumchi                                                                           |e. j. w. barber     |1999            |w. w. norton amp company|\n","|0399135782|the kitchen god's wife                                                                           |amy tan             |1991            |putnam pub group        |\n","|0425176428|what if the world's foremost military historians imagine what might have been                    |robert cowley       |2000            |berkley publishing group|\n","|0671870432|pleading guilty                                                                                  |scott turow         |1993            |audioworks              |\n","|0679425608|under the black flag the romance and the reality of life among the pirates                       |david cordingly     |1996            |random house            |\n","|074322678X|where you'll find me and other stories                                                           |ann beattie         |2002            |scribner                |\n","+----------+-------------------------------------------------------------------------------------------------+--------------------+----------------+------------------------+\n","only showing top 10 rows\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["270813"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["# export df_book into csv\n","df_book.repartition(1).write.csv(\"/content/drive/MyDrive/CSV/df_book.csv\", header = True)"],"metadata":{"id":"xMGWFTiSJD34","executionInfo":{"status":"ok","timestamp":1649798332324,"user_tz":-180,"elapsed":6287,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":["### **Users Table**"],"metadata":{"id":"zm3wFdTZNuUC"}},{"cell_type":"code","source":["# import csv\n","df_user = spark.read.option(\"delimiter\", \";\").csv('/content/drive/MyDrive/BDA/BigData/BX-Users.csv',inferSchema = True, header = True)\n","\n","# split the Location column\n","df_user = df_user.withColumn('city', split(df_user['Location'], ',').getItem(0)) \\\n","                 .withColumn('state', split(df_user['Location'], ',').getItem(1)) \\\n","                 .withColumn('country', split(df_user['Location'], ',').getItem(2)).drop('Location')\n","\n","# rename columns\n","df_user=df_user.withColumnRenamed('User-ID','user_id')\n","df_user=df_user.withColumnRenamed('Age','age')\n","\n","# filter irrelevant values from Age column into NULL\n","df_user = df_user.withColumn('age', when(col('age') < 10, None).otherwise(col('age')))\n","df_user = df_user.withColumn('age', when(col('age') > 120, None).otherwise(col('age')))\n","df_user = df_user.withColumn('age', when(col('age') == 'NULL', None).otherwise(col('age')))\n","\n","# turn NULL into -1\n","df_user = df_user.na.fill({\"age\": -1})\n","\n","# filter out non-digits characters from Age column\n","df_user = df_user.filter(~df_user.user_id.rlike('\\D+'))\n","\n","# filter out special characters\n","df_user = df_user.withColumn('city', regexp_replace('city', '[$&+,:;=?@#|<>^*()%!�\"]', ''))\n","df_user = df_user.withColumn('state', regexp_replace('state', '[$&+,:;=?@#|<>^*()%!�\"]', ''))\n","df_user = df_user.withColumn('country', regexp_replace('country', '[$&+,:;=?@#|<>^*()%!�\"]', ''))\n","\n","# lowercase all values\n","df_user.createOrReplaceTempView('df_user')\n","df_user = spark.table('df_user').withColumn('city', lower(col('city')))\n","\n","df_user.createOrReplaceTempView('df_user')\n","df_user = spark.table('df_user').withColumn('state', lower(col('state')))\n","\n","df_user.createOrReplaceTempView('df_user')\n","df_user = spark.table('df_user').withColumn('country', lower(col('country')))\n","\n","# trim trail and leading blank spaces\n","df_user = df_user.withColumn('user_id', trim(df_user.user_id))\n","df_user = df_user.withColumn('age', trim(df_user.age))\n","df_user = df_user.withColumn('city', trim(df_user.city))\n","df_user = df_user.withColumn('state', trim(df_user.state))\n","df_user = df_user.withColumn('country', trim(df_user.country))\n","\n","# convert data type to INT\n","df_user = df_user.withColumn(\"age\", df_user.age.cast('int'))\n","df_user = df_user.withColumn(\"user_id\", df_user.user_id.cast('int'))"],"metadata":{"id":"TNUpNubAOEmn","executionInfo":{"status":"ok","timestamp":1649798350855,"user_tz":-180,"elapsed":1360,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["# clean the Country column\n","df_user.createOrReplaceTempView('df_user')\n","df_user_temp = spark.sql(\"SELECT *, CASE \" +\n","\"WHEN country IN ('us','usa','u.s','u.s.a','united sates','united stated','united states of america','virginia','washington','west virginia','texas','ohio','south carolina','new york','california','dc','colorado','oregon','pennsylvania','florida','hawaii','illinois','indiana','massachusetts','missouri','minnesota','new jersey') THEN 'united states' \" +\n","\"WHEN country IN ('uk','scotland','wales','england') THEN 'united kingdom' \" +\n","\"WHEN country IN ('victoria','western australia') THEN 'australia' \" +\n","\"WHEN country IN ('catalunya','catalonia','espaa') THEN 'spain' \" +\n","\"WHEN country IN ('british columbia','ontario') THEN 'canada' \" +\n","\"WHEN country IN ('u.a.e') THEN 'united arab emirates' \" +\n","\"WHEN country IN ('italia','l`italia') THEN 'italy' \" +\n","\"WHEN country IN ('czech republic') THEN 'czechia' \" +\n","\"WHEN country IN ('deutschland') THEN 'germany' \" +\n","\"WHEN country IN ('la france') THEN 'france' \" +\n","\"WHEN country IN ('urugua') THEN 'uruguay' \" +\n","\"WHEN country IN ('yugoslavia') THEN null \" +\n","\"ELSE country END AS country_temp FROM df_user\").drop('country')\n","\n","# import the Countries CSV\n","df_countries = df_rating = spark.read.csv('/content/drive/MyDrive/BDA/BigData/Countries.csv', inferSchema = True, header = True)\n","\n","# Rename df_countries title\n","df_countries=df_countries.withColumnRenamed('name','countries')\n","\n","# lowercase df_countries values\n","df_countries.createOrReplaceTempView('df_countries')\n","df_countries = spark.table('df_countries').withColumn('countries', lower(col('countries')))\n","\n","df_countries.createOrReplaceTempView('df_countries')\n","df_countries = spark.table('df_countries').withColumn('region', lower(col('region')))\n","\n","# join (df_countries) table with (df_user_temp) table\n","df_join = df_user_temp.join(df_countries,df_user_temp.country_temp ==  df_countries.countries,\"inner\")\n","\n","# remove old country column and rename the new one\n","df_join = df_join.withColumnRenamed('countries','country').drop('country_temp')\n","\n","# convert to unknown sting column that has numbers\n","df_join = df_join.withColumn('city', regexp_replace('city', '[0-9]', 'unknown'))\n","df_join = df_join.withColumn('state', regexp_replace('state', '[0-9]', 'unknown'))\n","\n","# convert to unknown special character only\n","df_join.createOrReplaceTempView('df_join')\n","df_join = spark.sql(\"SELECT user_id,age,state,country,region, CASE WHEN city NOT REGEXP '^.*[a-zA-Z].*$' THEN 'unknown' ELSE city END AS city FROM df_join\")\n","\n","df_join.createOrReplaceTempView('df_join')\n","df_join = spark.sql(\"SELECT user_id,age,city,country,region, CASE WHEN state NOT REGEXP '^.*[a-zA-Z].*$' THEN 'unknown' ELSE state END AS state FROM df_join\")\n","\n","# convert empty values and null-string into NULL\n","Dict = {'n/a':None,'NULL':None,'':None,' ':None,'None':None,'none':None,'null':None}\n","df_join = df_join.replace(Dict,subset=['state','city'])\n","\n","# convert NULL to unknown\n","df_join = df_join.na.fill({\"city\": \"unknown\", \"state\": \"unknown\"})\n","\n","# reorganize the column order\n","df_join = df_join.select('user_id',\"age\",\"city\",\"state\",\"country\",\"region\")"],"metadata":{"id":"oHJgCWTZYAz8","executionInfo":{"status":"ok","timestamp":1649798355216,"user_tz":-180,"elapsed":794,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["# assign data frame back to df_user\n","df_user = df_join"],"metadata":{"id":"srs8bRV62O7t","executionInfo":{"status":"ok","timestamp":1649798360606,"user_tz":-180,"elapsed":286,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["df_user.show(10,truncate = False)\n","df_user.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YsHnINUQnWH","executionInfo":{"status":"ok","timestamp":1649798368219,"user_tz":-180,"elapsed":5131,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}},"outputId":"13d9b9b1-ef5a-4b53-8069-f02a7370d023"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+---+------------+---------------+--------------+-------------+\n","|user_id|age|city        |state          |country       |region       |\n","+-------+---+------------+---------------+--------------+-------------+\n","|1      |-1 |nyc         |new york       |united states |north america|\n","|2      |18 |stockton    |california     |united states |north america|\n","|3      |-1 |moscow      |yukon territory|russia        |central asia |\n","|4      |17 |porto       |v.n.gaia       |portugal      |europe       |\n","|5      |-1 |farnborough |hants          |united kingdom|europe       |\n","|6      |61 |santa monica|california     |united states |north america|\n","|7      |-1 |washington  |dc             |united states |north america|\n","|8      |-1 |timmins     |ontario        |canada        |north america|\n","|9      |-1 |germantown  |tennessee      |united states |north america|\n","|10     |26 |albacete    |wisconsin      |spain         |europe       |\n","+-------+---+------------+---------------+--------------+-------------+\n","only showing top 10 rows\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["272762"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","source":["# export df_user into csv\n","df_user.repartition(1).write.csv(\"/content/drive/MyDrive/CSV/df_user.csv\", header = True)"],"metadata":{"id":"mlcYrGzGBMJY","executionInfo":{"status":"ok","timestamp":1649798382085,"user_tz":-180,"elapsed":11805,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":["### **End Session**"],"metadata":{"id":"BUJnZARRoOLJ"}},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"V5cwYwzOoSqc","executionInfo":{"status":"ok","timestamp":1649798393270,"user_tz":-180,"elapsed":655,"user":{"displayName":"Yair Barel","userId":"02107957324727688835"}}},"execution_count":120,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"AgTr6K3upamR"},"execution_count":null,"outputs":[]}]}